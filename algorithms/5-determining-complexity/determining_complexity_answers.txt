1. O(1)
This algorithm accesses n as a constant. It immediately accesses n without any
further iterations required as n increases.

2. O(n)
This algorithm operates in linear time. Each time the function runs, it will access
every item in the collection, so as the collection size increases, the running time
will increase in a 1-to-1 proportion.

3. O(n * m)
The first thing I look at with this algorithm is the outer loop, which iterates for each entry
of the collection, or n times, so my baseline is a linear runtime. The inner loop performs
the same linear search on each array in the collection, so worst case scenario would be
on in which the last item in the last array was the largest value. Since the time is
based on two separate linear relationships, we would add those together.

4. O(n**2)
This algorithm makes recursive calls to itself. If we take the first two comparisons (to zero and one) to
be constants and ignore those in our classification, we can count the number of operations
in the recursive calls as n increases. Each time we increase n, we must call the method n times,
so the run time is quadratic.

A table of the number of operations relative to the size of n looks like this:
n | Operations
---------------------
0   1
1   2
2   4
3   8
4   14
5   24
6   40

5. O(n)
This algorithm runs in linear time. Each time n increases, the number of operations increases by (n - 1) * 4.

n | Operations
---------------------
0   0
1   0
2   4
3   8
4   12
5   16
6   20

6. O(n**2)
This algorithm has a while-loop, which runs in linear time, followed by two recursive calls
to each half of the partitioned collection, which I would understand to be a quadratic run time.
This would produce O(n + n**2), and since we are able to drop constants in this analysis, we can simply express
the runtime as O(n**2).
